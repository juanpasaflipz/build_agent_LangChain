{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNpmidufT59cUcvpR+41x2u",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanpasaflipz/build_agent_LangChain/blob/main/Agente_LangChan1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Agente AI\n",
        "Por sí mismos, los modelos de lenguaje no pueden realizar acciones; solo generan texto. Un caso de uso importante para LangChain es la creación de agentes.\n",
        "\n",
        "Los agentes son sistemas que utilizan un LLM como motor de razonamiento para determinar qué acciones tomar y cuáles deben ser las entradas para esas acciones.\n",
        "\n",
        "Los resultados de esas acciones pueden luego ser retroalimentados al agente, y este determinará si se necesitan más acciones o si está bien terminar.\n",
        "\n",
        "En este tutorial, construiremos un agente que puede interactuar con múltiples herramientas diferentes: una siendo una base de datos local, y la otra un motor de búsqueda. Podrá hacer preguntas a este agente, verlo usar herramientas y tener conversaciones con él.\n",
        "\n",
        "### Instalación\n",
        "Esto instalará los requisitos mínimos de LangChain. Gran parte del valor de LangChain proviene de integrarlo con varios proveedores de modelos, almacenes de datos, etc.\n",
        "\n",
        "Por defecto, las dependencias necesarias para hacer eso NO están instaladas. Tendrás que instalar las dependencias para integraciones específicas por separado."
      ],
      "metadata": {
        "id": "_VKD795dxJyc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zN_HZcok2Ftr"
      },
      "outputs": [],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "xkkNcPgCqmr_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LangSmith\n",
        "\n",
        "Muchas de las aplicaciones que se construyen con LangChain contienen pasos con múltiples invocaciones de llamadas a LLM. A medida que estas aplicaciones se vuelvan cada vez más complejas, se vuelve crucial poder inspeccionar exactamente lo que está sucediendo dentro de su cadena o agente. **La mejor manera de hacer esto es con LangSmith**"
      ],
      "metadata": {
        "id": "P1Px_YUW60Sw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "\n",
        "from google.colab import userdata;\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = userdata.get('LANGCHAIN_API_KEY')\n",
        "os.environ[\"LANGCHAIN_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\""
      ],
      "metadata": {
        "id": "skzxpuNJ4Lvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Definir la herramientas **(Tools)**"
      ],
      "metadata": {
        "id": "9a7uOZ-i6pFO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tavily\n",
        "\n",
        "Una herramienta integrada en LangChain para usar fácilmente el motor de búsqueda Tavily como herramienta. *Ten en cuenta que esto requiere una clave de API*."
      ],
      "metadata": {
        "id": "zwYaRUZsolGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_community"
      ],
      "metadata": {
        "id": "ExbLnOw68cgX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.tools.tavily_search import TavilySearchResults\n",
        "\n",
        "# Set the API key as an environment variable\n",
        "os.environ[\"TAVILY_API_KEY\"] = userdata.get('TAVILY_API_KEY')\n",
        "\n",
        "# Create an instance of TavilySearchResults\n",
        "search = TavilySearchResults()\n",
        "\n",
        "api_key = os.environ['TAVILY_API_KEY']\n",
        "\n",
        "# Use the TavilySearchResults with the API key\n",
        "tavily_search = TavilySearchResults(tavily_api_key=api_key, max_results=2)"
      ],
      "metadata": {
        "id": "3CZUQKAi5ZPT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Perform a search query\n",
        "search_results = tavily_search.invoke(\"quién se encuentra arriba en las encuestas para las elecciones presicenciales de México 2024?\")\n",
        "\n",
        "# Format and print the results in a human-readable way\n",
        "def print_human_readable_results(results):\n",
        "    for i, result in enumerate(results, 1):\n",
        "        print(f\"Result {i}: \\n\")\n",
        "        for key, value in result.items():\n",
        "            print(f\"  {key}: {value}\")\n",
        "        print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
        "\n",
        "print_human_readable_results(search_results)"
      ],
      "metadata": {
        "id": "I670PJAJ5urB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import display, Markdown\n",
        "\n",
        "# Perform a search query\n",
        "search_results = tavily_search.invoke(\"quién se encuantra arriba en las encuestas para las elecciones presicenciales de México 2024?\")\n",
        "\n",
        "\"\"\"\n",
        " def print_markdown_table(results):\n",
        "    if not results:\n",
        "        print(\"Sin resultados.\")\n",
        "        return\n",
        "\n",
        "    # Collect all keys to ensure a consistent table structure\n",
        "    all_keys = set()\n",
        "    for result in results:\n",
        "        all_keys.update(result.keys())\n",
        "\n",
        "    all_keys = sorted(all_keys)  # Sort keys for consistent order\n",
        "\n",
        "    # Print table header\n",
        "    header = \"| \" + \" | \".join(all_keys) + \" |\"\n",
        "    separator = \"| \" + \" | \".join(['---'] * len(all_keys)) + \" |\"\n",
        "    print(header)\n",
        "    print(separator)\n",
        "\n",
        "    # Print table rows\n",
        "    for result in results:\n",
        "        row = \"| \" + \" | \".join(str(result.get(key, \"\")) for key in all_keys) + \" |\"\n",
        "        print(row)\n",
        "    print(\"\\n\")\n",
        "\"\"\"\n",
        "\n",
        "#print_markdown_table(search_results) “””\n",
        "\n",
        "# Format the search results as a Markdown table\n",
        "def format_markdown_table(results):\n",
        "    if not results:\n",
        "        return \"No results found.\"\n",
        "\n",
        "    all_keys = set()\n",
        "    for result in results:\n",
        "        all_keys.update(result.keys())\n",
        "\n",
        "    all_keys = sorted(all_keys)\n",
        "\n",
        "    header = \"| \" + \" | \".join(all_keys) + \" |\"\n",
        "    separator = \"| \" + \" | \".join(['---'] * len(all_keys)) + \" |\"\n",
        "    rows = []\n",
        "\n",
        "    for result in results:\n",
        "        row = \"| \" + \" | \".join(str(result.get(key, \"\")) for key in all_keys) + \" |\"\n",
        "        rows.append(row)\n",
        "\n",
        "    return \"\\n\".join([header, separator] + rows)\n",
        "\n",
        "# Format the search results as a Markdown table\n",
        "search_result = format_markdown_table(search_results)\n",
        "\n",
        "# Display the search results as Markdown\n",
        "display(Markdown(search_result))"
      ],
      "metadata": {
        "id": "8pKo_nmJ5vrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Retriever\n",
        "*Herramienta #2*\n",
        "\n",
        "También crearemos un retriever (recuperador) sobre algunos datos propios."
      ],
      "metadata": {
        "id": "ifrs5msd6iSS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain_openai\n",
        "!pip install openai\n",
        "!pip install faiss-gpu"
      ],
      "metadata": {
        "id": "AIiBNEPzkv_p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import WebBaseLoader\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "loader = WebBaseLoader(\"https://docs.smith.langchain.com/overview\")\n",
        "docs = loader.load()\n",
        "documents = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000, chunk_overlap=200\n",
        ").split_documents(docs)\n",
        "vector = FAISS.from_documents(documents, OpenAIEmbeddings())\n",
        "retriever = vector.as_retriever()"
      ],
      "metadata": {
        "id": "ePO6eBHh5x0D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever.invoke(\"how to upload a dataset\")[0]"
      ],
      "metadata": {
        "id": "Vf5jQp5S6NLu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora que hemos poblado nuestro índice sobre el cual realizaremos la recuperación, podemos convertirlo fácilmente en una herramienta (el formato necesario para que un agente lo use correctamente).\n",
        "\n",
        "https://api.python.langchain.com/en/latest/tools/langchain_core.tools.create_retriever_tool.html"
      ],
      "metadata": {
        "id": "Ql2P9SfSsWN5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain.tools.retriever import create_retriever_tool"
      ],
      "metadata": {
        "id": "AKbCUEB66OA3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "retriever_tool = create_retriever_tool(\n",
        "    retriever,\n",
        "    \"langsmith_search\",\n",
        "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
        ")"
      ],
      "metadata": {
        "id": "q6L049QL6SdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tools (Herramientas)"
      ],
      "metadata": {
        "id": "rwEeOq_F6deH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tools = [search, retriever_tool]"
      ],
      "metadata": {
        "id": "OLBZjhTq6b5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Utilizando Modelos de Lenguaje\n",
        "\n",
        "\n",
        "*   OpenAI    `pip install -qU langchain-openai `\n",
        "*   Anthropic `pip install -qU langchain-anthropic `\n",
        "*   Google `pip install -qU langchain-google-vertexai `\n",
        "*   MistralAI `pip install -qU langchain-mistralai`\n",
        "*   y otros....\n",
        "\n"
      ],
      "metadata": {
        "id": "NA3C5GrDtucL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -qU langchain-openai"
      ],
      "metadata": {
        "id": "fcVbdzYrt1zS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "2dmMGfzbwkdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o\")"
      ],
      "metadata": {
        "id": "AltS2MiNxUcr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puedes llamar el modelo de lenguaje pasando una lista de mensajes. Por defecto, la respuesta es una cadena de contenido.\""
      ],
      "metadata": {
        "id": "mLG1JHjaxhiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "response = model.invoke([HumanMessage(content=\"hi!\")])\n",
        "response.content"
      ],
      "metadata": {
        "id": "uXL5-BoaxlbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora podemos ver cómo es habilitar que este modelo realice llamadas a herramientas. Para habilitar eso, usamos .bind_tools para darle al modelo de lenguaje conocimiento de estas herramientas."
      ],
      "metadata": {
        "id": "-f-ZQ4KOx_P_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_with_tools = model.bind_tools(tools)"
      ],
      "metadata": {
        "id": "QGx4Bq6tyAMs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora podemos llamar el modelo. Primero, llamémoslo con un mensaje normal y veamos cómo responde. Podemos observar tanto el campo de contenido como el campo de llamadas a herramientas."
      ],
      "metadata": {
        "id": "9PXOd-U5yKl2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = model_with_tools.invoke([HumanMessage(content=\"Hi!\")])\n",
        "\n",
        "print(f\"ContentString: {response.content}\")\n",
        "print(f\"ToolCalls: {response.tool_calls}\")"
      ],
      "metadata": {
        "id": "5ppYutkLyNzC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ahora, intentemos llamarlo con una entrada que espere que se llame a una herramienta."
      ],
      "metadata": {
        "id": "tyzvFwPQyY7o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = model_with_tools.invoke([HumanMessage(content=\"quién se encuantra arriba en las encuestas para las elecciones presicenciales de México 2024?\")])\n",
        "\n",
        "print(f\"ContentString: {response.content}\")\n",
        "print(f\"ToolCalls: {response.tool_calls}\")"
      ],
      "metadata": {
        "id": "-xDuKZfxybJR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "¡Podemos ver que ahora no hay contenido, pero sí hay una llamada a una herramienta! Quiere que llamemos a la herramienta Tavily Search.\n",
        "\n",
        "*Esto aún no está llamando a esa herramienta; solo nos lo está indicando. Para llamarla, necesitamos crear nuestro agente.*"
      ],
      "metadata": {
        "id": "CEz95teRy6bR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Crear el agente\n",
        "Ahora que hemos definido las herramientas y el LLM, podemos crear el agente. **Usaremos LangGraph para construir el agente**.\n",
        "\n",
        "Actualmente estamos utilizando una interfaz de alto nivel para construir el agente, pero lo bueno de LangGraph es que esta interfaz de alto nivel está respaldada por una API de bajo nivel altamente controlable en caso de que desee modificar la lógica del agente.  --- *Esto quiere decir que en este momento se está utilizando una interfaz sencilla y fácil de usar para crear el agente. Sin embargo, LangGraph ofrece la ventaja de que esta interfaz sencilla está soportada por una API más avanzada y detallada, que permite un mayor control y personalización si se desea modificar el funcionamiento interno del agente.*\n",
        "\n",
        "Ahora, podemos inicializar el agente con el LLM y las herramientas.\n",
        "\n",
        "Ten en cuenta que solamente estamos pasando el modelo, no el modelo con herramientas. **Eso es porque create_tool_calling_executor llamará a .bind_tools por nosotros automáticamente**."
      ],
      "metadata": {
        "id": "B_RQQDPlzCyc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langgraph"
      ],
      "metadata": {
        "id": "2a0PMaRsz67K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.prebuilt import chat_agent_executor\n",
        "\n",
        "agent_executor = chat_agent_executor.create_tool_calling_executor(model, tools)"
      ],
      "metadata": {
        "id": "H3apAu3PzvnI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ejecutar el agente\n",
        "¡Ahora podemos ejecutar el agente en algunas consultas! Ten en cuenta que, por ahora, todas son consultas sin estado (no recordará interacciones previas).\n",
        "\n",
        "Nota que el agente devolverá el estado final al final de la interacción (lo que incluye cualquier entrada; más adelante veremos cómo obtener solo las salidas).\n",
        "\n",
        "Primero, veamos cómo responde cuando no hay necesidad de llamar a una herramienta:"
      ],
      "metadata": {
        "id": "DNtl64RW0Iyq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent_executor.invoke({\"messages\": [HumanMessage(content=\"hi!\")]})\n",
        "\n",
        "response[\"messages\"]"
      ],
      "metadata": {
        "id": "RYJgNiMm0R_k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Para ver exactamente lo que está sucediendo internamente (y para asegurarnos de que no está llamando a una herramienta), podemos echar un vistazo a [LangSmith Trace](https://smith.langchain.com/public/r)*"
      ],
      "metadata": {
        "id": "yIo7FBpB0gsE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "response = agent_executor.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"quién se encuantra arriba en las encuestas para las elecciones presicenciales de México 2024?\")]}\n",
        ")\n",
        "response[\"messages\"]"
      ],
      "metadata": {
        "id": "yiNF0dl81C3N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "XFc9V8pj1Tyh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transmisión de mensajes (Streaming)\n",
        "Hemos visto cómo se puede llamar al agente con .invoke para obtener una respuesta final. Si el agente está ejecutando múltiples pasos, eso puede tardar un tiempo. Para mostrar el progreso intermedio, podemos transmitir los mensajes a medida que ocurren."
      ],
      "metadata": {
        "id": "i3HCZHyJ2Ej9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in agent_executor.stream(\n",
        "    {\"messages\": [HumanMessage(content=\"quién se encuentra arriba en las encuestas para las elecciones presicenciales de México 2024?\")]}\n",
        "):\n",
        "    print(chunk)\n",
        "    print(\"----\")"
      ],
      "metadata": {
        "id": "XTIV7Vkt2EJI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Transmisión de tokens\n",
        "Además de transmitir mensajes, también es útil transmitir tokens. Podemos hacer esto con el método .astream_events."
      ],
      "metadata": {
        "id": "IIqPoGyU2jwt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "async for event in agent_executor.astream_events(\n",
        "    {\"messages\": [HumanMessage(content=\"quién se encuentra arriba en las encuestas para las elecciones presicenciales de México 2024?\")]}, version=\"v1\"\n",
        "):\n",
        "    kind = event[\"event\"]\n",
        "    if kind == \"on_chain_start\":\n",
        "        if (\n",
        "            event[\"name\"] == \"Agent\"\n",
        "        ):  # Was assigned when creating the agent with `.with_config({\"run_name\": \"Agent\"})`\n",
        "            print(\n",
        "                f\"Starting agent: {event['name']} with input: {event['data'].get('input')}\"\n",
        "            )\n",
        "    elif kind == \"on_chain_end\":\n",
        "        if (\n",
        "            event[\"name\"] == \"Agent\"\n",
        "        ):  # Was assigned when creating the agent with `.with_config({\"run_name\": \"Agent\"})`\n",
        "            print()\n",
        "            print(\"--\")\n",
        "            print(\n",
        "                f\"Done agent: {event['name']} with output: {event['data'].get('output')['output']}\"\n",
        "            )\n",
        "    if kind == \"on_chat_model_stream\":\n",
        "        content = event[\"data\"][\"chunk\"].content\n",
        "        if content:\n",
        "            # Empty content in the context of OpenAI means\n",
        "            # that the model is asking for a tool to be invoked.\n",
        "            # So we only print non-empty content\n",
        "            print(content, end=\"|\")\n",
        "    elif kind == \"on_tool_start\":\n",
        "        print(\"--\")\n",
        "        print(\n",
        "            f\"Starting tool: {event['name']} with inputs: {event['data'].get('input')}\"\n",
        "        )\n",
        "    elif kind == \"on_tool_end\":\n",
        "        print(f\"Done tool: {event['name']}\")\n",
        "        print(f\"Tool output was: {event['data'].get('output')}\")\n",
        "        print(\"--\")"
      ],
      "metadata": {
        "id": "aLHlESuJ2p-W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Agregar memoria\n",
        "Como mencioné anteriormente, este agente no tiene estado, *es stateless*. Esto significa que no recuerda interacciones previas. Para darle memoria, necesitamos pasar un checkpointer. Al pasar un checkpointer, también tenemos que pasar un `thread_id` al invocar al agente (para que sepa de qué hilo/conversación debe reanudar)."
      ],
      "metadata": {
        "id": "BdRfUeGe2-bi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langgraph.checkpoint.sqlite import SqliteSaver\n",
        "\n",
        "memory = SqliteSaver.from_conn_string(\":memory:\")"
      ],
      "metadata": {
        "id": "FPSd282W3gWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "agent_executor = chat_agent_executor.create_tool_calling_executor(\n",
        "    model, tools, checkpointer=memory\n",
        ")\n",
        "\n",
        "config = {\"configurable\": {\"thread_id\": \"abc123\"}}"
      ],
      "metadata": {
        "id": "oOa52Dqk3jT7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in agent_executor.stream(\n",
        "    {\"messages\": [HumanMessage(content=\"hola soy juan!\")]}, config\n",
        "):\n",
        "    print(chunk)\n",
        "    print(\"----\")"
      ],
      "metadata": {
        "id": "tNLVu8NN3nPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for chunk in agent_executor.stream(\n",
        "    {\"messages\": [HumanMessage(content=\"whats my name?\")]}, config\n",
        "):\n",
        "    print(chunk)\n",
        "    print(\"----\")"
      ],
      "metadata": {
        "id": "pRzoU6o63w5z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}